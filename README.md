Dataset Information

We'll work with the metadata.csv file from the CORD-19 dataset, which contains information about COVID-19 research papers. The file includes:

Paper titles and abstracts

Publication dates

Authors and journals

Source information

You can download the dataset from Kaggle: (metadata.csv) = 1.51GB 
https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge

# CORD-19 Data Analysis with Streamlit

This repository contains a simple analysis of the CORD-19 research dataset metadata and a Streamlit application to visualize the findings.

## Project Structure

*   `data_exploration.py`: Script for initial data loading and exploration.
*   `data_cleaning.py`: Script to clean the metadata (handle missing values, convert dates, create new columns).
*   `analysis_and_viz.py`: Script to perform basic analysis and generate static visualizations.
*   `app.py`: The main Streamlit application file.
*   `metadata.csv`: The raw dataset file (downloaded from Kaggle).
*   `cleaned_data.csv`: The output file from `data_cleaning.py`.
*   `visualizations/`: Folder containing static plots generated by `analysis_and_viz.py`.
*   `requirements.txt`: Lists the Python dependencies.
*   `README.md`: This file

## Getting Started

1.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
    (Ensure `requirements.txt` includes `pandas`, `matplotlib`, `seaborn`, `streamlit`, `wordcloud`)

2.  **Download Data**:
    *   Download the `metadata.csv` file from [Kaggle CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).
    *   Place the `metadata.csv` file in the project root directory.

3.  **Run Data Cleaning**:
    ```bash
    python data_cleaning.py
    ```
    This creates the `cleaned_data.csv` file.

4.  **(Optional) Run Analysis and Visualization**:
    ```bash
    python analysis_and_viz.py
    ```
    This generates plots in the `visualizations/` folder.

5.  **Run the Streamlit App**:
    ```bash
    streamlit run app.py
    ```
    This starts the web application in your browser.

## Findings (Brief Report)

*   **Data Size**: The original metadata file contains [X] rows and [Y] columns.
*   **Missing Data**: Columns like [list key columns with missing data] had significant missing values, which were handled by [describe your strategy, e.g., dropping rows, filling with empty strings].
*   **Temporal Trends**: The number of publications related to the topics in the metadata increased sharply around [year], likely corresponding to the onset of the pandemic.
*   **Key Sources**: Journals/publication sources like [list top sources found] were prominent publishers of these papers.
*   **Common Terms**: Frequent words in titles included [list some common terms found in word cloud/analysis], reflecting common research themes.

## Reflection

*   **Challenges**: Handling the large size of the dataset efficiently, dealing with inconsistent date formats, and choosing appropriate methods for text analysis (word clouds are basic) were initial hurdles.
*   **Learning**: This project reinforced skills in data loading with `pandas`, data cleaning techniques, creating static visualizations with `matplotlib`/`seaborn`, and building interactive web apps with `Streamlit`. It highlighted the importance of exploratory data analysis before diving into complex analysis.
